{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Nice Ride Minnesota 2017\n",
    "\n",
    "As a Minnesotan, I wanted to do a project related to where I'm from. I found this dataset on Kaggle and thought it would make a fun challenge. Nice Ride is a community biking service where bikes can be picked up from one location and dropped off at another. The idea is to create healthier communities and increase transporation options for people living in the Twin Cities.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The datasets provided include:\n",
    "* `trip_history` : A trip history list for each ride taken by a customer\n",
    "* `station_locations` : A list of all the stations and their locations\n",
    "* `weather` : A basic weather report for each day\n",
    "\n",
    "### Questions\n",
    "\n",
    "Given these datasets, I'd like answers to the following questions:\n",
    "* Do stations align with neighboorhoods?\n",
    "* Can predictions about the number of rides per day based weather be accomplished with 80% or more accuracy?\n",
    "* Can the speed of trips between stations be calculated?\n",
    "\n",
    "### Plan\n",
    "\n",
    "My plan is to inspect the data for errors and issues and look at some basic statistics in the EDA process, then start asking questions of the data.\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "* [Nice Ride Minnesota 2017, Kaggle Dataset](https://www.kaggle.com/brendanhasz/nice-ride-mn-2017/version/2#WeatherDailyMinneapolis2017.csv)\n",
    "* [Minneapolis Boundaries](http://opendata.minneapolismn.gov/datasets/89f1a70c0cf24d7692e2d02fdf8f4e47_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings\n",
    "\n",
    "* `is_first_time_run`: when set to `True`, the packages not commonly included in a standard Jupyter instance will be filed. \n",
    "* `months`: A list of months for cleanly labling the charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to download packages that aren't common \n",
    "is_first_time_run = True\n",
    "\n",
    "# Used to put the months on the axis\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Use this toggle to show or hide the code blocks for easier reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Some installations of jupyter reset every time the notebook is shut down\n",
    "if is_first_time_run:\n",
    "    !pip install folium\n",
    "    !pip install kml2geojson\n",
    "    !pip install shapely\n",
    "    !pip install plotnine\n",
    "    !pip install mizani\n",
    "    !pip install geopy\n",
    "    is_first_time_run = False\n",
    "\n",
    "import folium\n",
    "import json\n",
    "import kml2geojson\n",
    "import warnings\n",
    "import calendar\n",
    "import geopy.distance\n",
    "\n",
    "from shapely.geometry import shape, Point, asShape\n",
    "from mizani.breaks import date_breaks\n",
    "from mizani.formatters import date_format\n",
    "from plotnine import *\n",
    "from IPython.display import HTML\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# Import modeling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Clean up presentation\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# Clean up the notebook by hiding the code blocks\n",
    "def show_hide_code_toggle():\n",
    "    # Code from:\n",
    "    # http://stackoverflow.com/questions/27934885/how-to-hide-code-from-cells-in-ipython-notebook-visualized-with-nbviewer\n",
    "    \n",
    "    html = HTML('''<script>\n",
    "    code_show=true; \n",
    "    function code_toggle() {\n",
    "    if (code_show){\n",
    "        $('div.input').hide();\n",
    "    } else {\n",
    "        $('div.input').show();\n",
    "    }\n",
    "    code_show = !code_show\n",
    "    } \n",
    "    $( document ).ready(code_toggle);\n",
    "    </script>\n",
    "    <form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')\n",
    "    \n",
    "    return(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_hide_code_toggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df_stations = pd.read_csv('station_locations.csv')\n",
    "df_trips = pd.read_csv('trip_history.csv', dtype = 'unicode')\n",
    "df_weather = pd.read_csv('weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Minneapolis boundaries and convert them to a useable format by folium\n",
    "kml2geojson.main.convert('msvcGIS_MinneapolisCityLimits.kml', 'json_data')\n",
    "\n",
    "with open('json_data/msvcGIS_MinneapolisCityLimits.geojson') as json_data:\n",
    "    json_boundaries = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "I started by getting to know the data a little bit. \n",
    "\n",
    "### Station List Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_shape_description(df, name):\n",
    "    '''\n",
    "    Builds a string with a pretty version of a dataframe's shape (rows and columns).\n",
    "    \n",
    "    Parameters:\n",
    "    df (dataframe): The dataframe under inspection\n",
    "    name (str): For identification purposes, a user identifiable name should be entered\n",
    "    \n",
    "    Returns:\n",
    "    str: A line of descriptive text\n",
    "    '''\n",
    "    \n",
    "    shape = df.shape\n",
    "    rows = shape[0]\n",
    "    columns = shape[1]\n",
    "    \n",
    "    return(name + ' has ' + str(rows) + ' rows and ' + str(columns) + ' columns.')\n",
    "\n",
    "def df_na_count(df):\n",
    "    '''\n",
    "    Builds a string with the number of NaN/NA elements in a dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    df (dataframe): The dataframe under inspection\n",
    "    \n",
    "    Returns:\n",
    "    str: A line of descriptive text\n",
    "    '''\n",
    "    return(str(df.isna().sum()))\n",
    "\n",
    "def print_df_summary(df, name):\n",
    "    '''\n",
    "    Builds a complete summary of a dataframe for easy printing.\n",
    "    \n",
    "    Parameters:\n",
    "    df (dataframe): The dataframe that needs a summary\n",
    "    name (str): A friendly description of the dataframe for identification purpses while printing\n",
    "    '''\n",
    "    print(df_shape_description(df, name))\n",
    "    print('')\n",
    "    print(df.head())\n",
    "    print('')\n",
    "    print(name + ' has the following nulls:')\n",
    "    print(df_na_count(df))\n",
    "    print('')\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_df_summary(df_stations, 'df_stations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data includes `202` stations with their identification number, station name, coordinate location, and total number of bikes that can be docked. Next, I plotted the locations of each station on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_map(df, should_scale = False, should_cluster = False, is_simple = False):\n",
    "    '''\n",
    "    Builds a map of Minneapolis with latitude and longitude coordinates present in a dataframe represented as dots.\n",
    "    \n",
    "    Paramters:\n",
    "    df (dataframe): The dataframe under inspection. Should have a \"Latitude\" and \"Longitude\" column\n",
    "    should_scale (Bool): Defaults to False. If True, the point on the map representing the coordinates will scale in size based on the \"Scale\" column\n",
    "    should_cluster (Bool): Defaults to False. If True, color values will be added to the points based on the \"Cluster\" column, cannot use if there are more than 10 clusters\n",
    "    is_simple (Bool): Defaults to False. If True, a dataframe with just \"Latitude\" and \"Longitude\" values can be passed, all other customization (like popup text) are ignored\n",
    "    '''\n",
    "    \n",
    "    # Map design inspriation from\n",
    "    # https://www.kaggle.com/shivamb/4-1-analysis-measuring-equity-minneapolis-pd/notebook\n",
    "    \n",
    "    # Build map frame\n",
    "    zoom = 0\n",
    "    if should_scale:\n",
    "        zoom = 12\n",
    "    else:\n",
    "        zoom = 11.455\n",
    "    \n",
    "    # Settings\n",
    "    cluster_colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange', 'pink', 'white', 'brown', 'fuschia']\n",
    "    mpls_coords = [44.986656, -93.258133]\n",
    "    map_mpls = folium.Map(mpls_coords, zoom_start = zoom, tiles = 'CartoDB dark_matter')\n",
    "\n",
    "    # Add station dot to map\n",
    "    for i, row in df.iterrows():\n",
    "        \n",
    "        # Build contextually relevant text for the station popup\n",
    "        popup_text = ''\n",
    "        if should_scale:\n",
    "            popup_text = row['Name'] + ': ' + str(row['RideCount']) + ' total rides.'\n",
    "        elif is_simple:\n",
    "            popup_text = ''\n",
    "        else: \n",
    "            popup_text = row['Name'] + ': ' + str(row['Total docks']) + ' bike docks.'\n",
    "            \n",
    "        lat_long = [float(row['Latitude']), float(row['Longitude'])]\n",
    "        \n",
    "        if should_scale:\n",
    "            folium.CircleMarker(lat_long, radius = row['Scale'], color = 'red', fill_color = 'red', fill_opacity = 0.5, popup = popup_text).add_to(map_mpls)\n",
    "        else:\n",
    "            if should_cluster:\n",
    "                color_name = cluster_colors[row['Cluster']]\n",
    "                folium.CircleMarker(lat_long, radius = 3, color = color_name, popup = popup_text).add_to(map_mpls)\n",
    "            else:\n",
    "                folium.CircleMarker(lat_long, radius = 3, color = 'red', popup = popup_text).add_to(map_mpls)\n",
    "\n",
    "    if should_scale == False and should_cluster == False:\n",
    "        folium.GeoJson(json_boundaries, style_function = lambda feature: {\n",
    "            'fillColor': 'white', 'color': 'white', 'weight': 1, 'fillOpacity': 0.1 \n",
    "        }).add_to(map_mpls)\n",
    "    else:\n",
    "        folium.GeoJson(json_boundaries, style_function = lambda feature: {\n",
    "            'fillColor': 'white', 'color': 'white', 'weight': 1, 'fillOpacity': 0.0 \n",
    "        }).add_to(map_mpls)\n",
    "    \n",
    "    return(map_mpls)\n",
    "    \n",
    "# Display map    \n",
    "map_mpls = build_map(df_stations)\n",
    "map_mpls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, I want to focus on stops within Minneapolis, so I'll remove the stops not in the Minneapolis boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_in_minneapolis = []\n",
    "list_in_saint_paul = []\n",
    "\n",
    "# Extract Minneapolis boundary shape\n",
    "shape_minneapolis = None\n",
    "for f in json_boundaries['features']:    \n",
    "    shape_minneapolis = shape(f['geometry'])\n",
    "\n",
    "# Filter if lat/long is within boundary shape\n",
    "for i, row in df_stations.iterrows():\n",
    "    polygon = shape(shape_minneapolis)\n",
    "    point = Point(row['Longitude'], row['Latitude'])\n",
    "    \n",
    "    if polygon.contains(point):\n",
    "        list_in_minneapolis.append(row['Number'])\n",
    "    else:\n",
    "        list_in_saint_paul.append(row['Number'])\n",
    "        \n",
    "print('Minneapolis has ' +  str(len(list_in_minneapolis)) + ' stations.')\n",
    "print('Saint Paul has ' + str(len(list_in_saint_paul)) + ' stations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the Saint Paul stops, I looked at the description again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_mpls = df_stations[df_stations['Number'].isin(list_in_minneapolis)]\n",
    "print_df_summary(df_stations_mpls, 'df_stations, Minneapolis only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There, now our stations are limited to just Minneapolis, but let's make sure with a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_mpls = build_map(df_stations_mpls)\n",
    "map_mpls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks right to me!\n",
    "\n",
    "### Weather Data Inspection\n",
    "\n",
    "Weather data was also included and should make for some interesting predictions. Some formatting of the dates was performed in order to do grouping that `ggplot` can work with. In particular, the month was extracted and translated into the calendar name, as well as a month/year grouping for aggregation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Datatypes\n",
    "df_weather['DATE'] = df_weather['DATE'].astype('datetime64[ns]')\n",
    "df_weather['PRCP'] = df_weather['PRCP'].astype(float)\n",
    "\n",
    "# Add month \n",
    "lambda_get_date = lambda x: str(x).split('-')[1] + '/' + str(x).split('-')[0]\n",
    "df_weather['MONTH'] = df_weather['DATE'].apply(lambda_get_date)\n",
    "\n",
    "print_df_summary(df_weather, 'df_weather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'll start by looking at the distributions of the single variables. Starting with precipitation.\n",
    "\n",
    "#### Precipitation\n",
    "\n",
    "The total amount of precipitation per day was measured in inches and included in this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (ggplot(df_weather) +\n",
    "            geom_histogram(aes(x = 'PRCP'), \n",
    "                          bins = 8,\n",
    "                          color = 'white',\n",
    "                          fill = 'cornflowerblue') +\n",
    "        ggtitle('Precipitation Histogram') +\n",
    "        xlab('Precipitation (in)') +\n",
    "        ylab('Frequency')\n",
    "       )\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_only_prcp = df_weather[df_weather['PRCP'] > 0]\n",
    "\n",
    "plot = (ggplot(df_weather_only_prcp) +\n",
    "            geom_histogram(aes(x = 'PRCP'), \n",
    "                           bins = 8,\n",
    "                           color = 'white',\n",
    "                           fill = 'cornflowerblue') +\n",
    "            facet_wrap('~MONTH', ncol = 4) +\n",
    "            ggtitle('Precipitation Histogram by Month (0 Values Removed)') +\n",
    "            xlab('Precipitation (in)') +\n",
    "            ylab('Frequency')\n",
    "       )\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that precipitation is very positively skewed. To make the by month plot more interesting, the zero values were removed to highlight actual precipitation amounts. \n",
    "\n",
    "#### Max Temperature\n",
    "\n",
    "The maximim recorded temperature for that day was also measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (ggplot(df_weather) +\n",
    "            geom_histogram(aes(x = 'TMAX'),\n",
    "                          color = 'white',\n",
    "                          fill = 'cornflowerblue') +\n",
    "        ggtitle('Max Temperature Histogram') +\n",
    "        xlab('Temperature (F°)') +\n",
    "        ylab('Frequency')\n",
    "       )\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max temperatures were then broken out by month and facet wrapped as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (ggplot(df_weather) +\n",
    "            geom_histogram(aes(x = 'TMAX'), \n",
    "                           bins = 8,\n",
    "                           color = 'white',\n",
    "                           fill = 'cornflowerblue') +\n",
    "            facet_wrap('~MONTH', ncol = 4) +\n",
    "            ggtitle('Max Temperature by Month') +\n",
    "            xlab('Temperature (F°)') +\n",
    "            ylab('Frequency')\n",
    "       )\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The monthly temperatures were averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max_average_temp = df_weather.groupby('MONTH')['TMAX'].mean()\n",
    "df_max_average_temp = df_max_average_temp.reset_index()\n",
    "df_max_average_temp.columns = ['MONTH', 'TMAX_AVG']\n",
    "\n",
    "# geom_line connects on values it understands\n",
    "df_max_average_temp.insert(0, 'MONTH_ID', range(1, 1 + len(df_max_average_temp)))\n",
    "\n",
    "plot = (ggplot(df_max_average_temp,\n",
    "              aes(x = 'MONTH_ID', y = 'TMAX_AVG')) +\n",
    "        geom_point() +\n",
    "        geom_line(color = 'cornflowerblue') + \n",
    "        scale_x_continuous(labels = months[:-1],\n",
    "                        breaks = df_max_average_temp['MONTH_ID'].tolist(),\n",
    "                        limits = [1, 12]) +\n",
    "        ggtitle('2017 Average Monthly Maximum Temperature') +\n",
    "        xlab('Date') +\n",
    "        ylab('Temperature (F°)')\n",
    ")\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the monthly distributions and average temperatures, the seasonal trends are visible. Neat! Next, the same treatments were applied to the minimum temperatures.\n",
    "\n",
    "#### Minimum Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (ggplot(df_weather) +\n",
    "            geom_histogram(aes(x = 'TMIN'),\n",
    "                          color = 'white',\n",
    "                          fill = 'cornflowerblue') +\n",
    "        ggtitle('Minimum Temperature Histogram') +\n",
    "        xlab('Temperature (F°)') +\n",
    "        ylab('Frequency')\n",
    "       )\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (ggplot(df_weather) +\n",
    "            geom_histogram(aes(x = 'TMIN'), \n",
    "                           bins = 8,\n",
    "                           color = 'white',\n",
    "                           fill = 'cornflowerblue') +\n",
    "            facet_wrap('~MONTH', ncol = 4) +\n",
    "            ggtitle('Minimum Temperature by Month') +\n",
    "            xlab('Temperature (F°)') +\n",
    "            ylab('Frequency')\n",
    "       )\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minimum_average_temp = df_weather.groupby('MONTH')['TMIN'].mean()\n",
    "df_minimum_average_temp = df_minimum_average_temp.reset_index()\n",
    "df_minimum_average_temp.columns = ['MONTH', 'TMIN_AVG']\n",
    "\n",
    "# geom_line connects on values it understands\n",
    "df_minimum_average_temp.insert(0, 'MONTH_ID', range(1, 1 + len(df_minimum_average_temp)))\n",
    "lambda_month_lookup = lambda x: calendar.month_abbr[x]\n",
    "df_minimum_average_temp['MONTH_NAME'] = df_minimum_average_temp['MONTH_ID'].apply(lambda_month_lookup)\n",
    "\n",
    "plot = (ggplot(df_minimum_average_temp,\n",
    "              aes(x = 'MONTH_ID', y = 'TMIN_AVG')) +\n",
    "        geom_point() +\n",
    "        geom_line(color = 'cornflowerblue') + \n",
    "        scale_x_continuous(labels = months[:-1],\n",
    "                        breaks = df_minimum_average_temp['MONTH_ID'].tolist(),\n",
    "                        limits = [1, 12]) +\n",
    "        ggtitle('2017 Average Monthly Minimum Temperature') +\n",
    "        xlab('Date') +\n",
    "        ylab('Temperature (F°)')\n",
    ")\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I wanted to look at the these variables over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (ggplot(df_weather) + \n",
    "    geom_point(aes(x = 'DATE', y = 'PRCP')) + \n",
    "    scale_x_date(breaks = date_breaks('1 months'), labels = months) +\n",
    "    ggtitle('2017 Minneapolis Precipitation') +\n",
    "    xlab('Date') +\n",
    "    ylab('Precipitation (in)')\n",
    ")\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's quite a few days with no recorded precipitation. Let's remove those values and see if there are any apparent trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (ggplot(df_weather_only_prcp) + \n",
    "    geom_point(aes(x = 'DATE', y = 'PRCP')) + \n",
    "    geom_smooth(aes(x = 'DATE', y = 'PRCP'), color = 'blue') +\n",
    "    scale_x_date(breaks = date_breaks('1 months'), labels = months) +\n",
    "    ggtitle('2017 Minneapolis Precipitation') +\n",
    "    xlab('Date') +\n",
    "    ylab('Precipitation (in)')\n",
    ")\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that was anticlimactic. Looks like with simple inspection, no obvious trends stand out. Let's plot some more of the features over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prcp_sum = df_weather.groupby(['MONTH'])['PRCP'].sum()\n",
    "df_prcp_sum = df_prcp_sum.reset_index()\n",
    "df_prcp_sum.columns = ['MONTH', 'TOTAL']\n",
    "\n",
    "plot = (ggplot(df_prcp_sum) +\n",
    "        geom_bar(aes(x = 'MONTH', y = 'TOTAL'), \n",
    "                 stat = 'identity',\n",
    "                 color = 'white',\n",
    "                 fill = 'cornflowerblue') +\n",
    "        scale_x_date(breaks = date_breaks('1 months'), labels = months[:-1]) +\n",
    "        ggtitle('2017 Minneapolis Precipitation Total') +\n",
    "        xlab('Date') +\n",
    "        ylab('Precipitation (in)')\n",
    ")\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (ggplot(df_weather) +\n",
    "    geom_smooth(aes(x = 'DATE', y = 'TMAX'), color = 'red') +\n",
    "    geom_smooth(aes(x = 'DATE', y = 'TMIN'), color = 'blue') +\n",
    "    geom_point(aes(x = 'DATE', y = 'TMAX'), color = 'red', alpha = 0.1) +\n",
    "    geom_point(aes(x = 'DATE', y = 'TMIN'), color = 'blue', alpha = 0.1) +\n",
    "    scale_x_date(breaks = date_breaks('1 months'), labels = months) +\n",
    "    ggtitle('2017 Minneapolis Temperatures') +\n",
    "    xlab('Date') +\n",
    "    ylab('Temperature (F°)')\n",
    ")\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_average_temps = df_minimum_average_temp.merge(df_max_average_temp)\n",
    "\n",
    "# Beautify the degrees for labeling \n",
    "lambda_apply_degree = lambda x: x + '°'\n",
    "df_average_temps['TMIN_AVG_L'] = df_average_temps['TMIN_AVG'].round(0).astype(int).astype(str).apply(lambda_apply_degree)\n",
    "df_average_temps['TMAX_AVG_L'] = df_average_temps['TMAX_AVG'].round(0).astype(int).astype(str).apply(lambda_apply_degree)\n",
    "\n",
    "plot = (ggplot(df_average_temps) +\n",
    "        geom_point(aes(x = 'MONTH_ID', y = 'TMIN_AVG'), color = 'blue') +\n",
    "        geom_text(aes(x = 'MONTH_ID', y = 'TMIN_AVG', label = 'TMIN_AVG_L'), nudge_y = -5, nudge_x = 0.1) +\n",
    "        geom_point(aes(x = 'MONTH_ID', y = 'TMAX_AVG'), color = 'red') +\n",
    "        geom_text(aes(x = 'MONTH_ID', y = 'TMAX_AVG', label = 'TMAX_AVG_L'), nudge_y = 5, nudge_x = 0.1) +\n",
    "        geom_line(aes(x = 'MONTH_ID', y = 'TMIN_AVG'), color = 'blue') +\n",
    "        geom_line(aes(x = 'MONTH_ID', y = 'TMAX_AVG'), color = 'red') +\n",
    "        scale_x_continuous(labels = months[:-1],\n",
    "                        breaks = df_average_temps['MONTH_ID'].tolist(),\n",
    "                        limits = [1, 12]) +\n",
    "        ggtitle('2017 Average Monthly Temperatures') +\n",
    "        xlab('Date') +\n",
    "        ylab('Temperature (F°)')\n",
    ")\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the average temperature in October is between 40° and 58°, but there was a significant amount of rain compared to the surrounding months. I remember getting blizzards in October, not rain!\n",
    "\n",
    "#### Correlation\n",
    "\n",
    "Okay, next, a correlation matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation code from:\n",
    "# https://stackoverflow.com/questions/29432629/correlation-matrix-using-pandas#29432741\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "corr = df_weather.corr()\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's also anticlimactic. It makes sense that the maximum and minimum temperatures correlate with each other. Precipitation appears to negatively correlated with temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trip Summaries Data Inspection\n",
    "\n",
    "This a significantly larger dataset than the weather and station data. It contains the start and end times, with the time elapsed conveniently calculated for me. Other attributes include the station name and number, the latter can be linked to the station database, and whether or not the customer that used the bike was a member of the Nice Ride service or not. \n",
    "\n",
    "As with the weather data, some additional transformations were applied to make it easier to group the trips together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_date_string(row):\n",
    "    '''\n",
    "    Takes american formatted dates ('MM/DD/YYYY') and converts them to a standard YYYY-MM-DD format\n",
    "    Intended to be \"apply()-ed\" to each row of a dataframe column.\n",
    "    \n",
    "    Parameters:\n",
    "    row (str): A cell of a dataframe column in the 'MM/DD/YYYY' format\n",
    "    \n",
    "    Returns:\n",
    "    str: The same date passed in but in YYYY-MM-DD format\n",
    "    '''\n",
    "    date_components = str(row).split('/')\n",
    "    \n",
    "    corrected_date_components = []\n",
    "    for item in date_components:\n",
    "        if len(item) == 1:\n",
    "            new_component = '0' + str(item)\n",
    "            corrected_date_components.append(new_component)\n",
    "        else:\n",
    "            corrected_date_components.append(item)\n",
    "            \n",
    "    corrected_date_ordering = [corrected_date_components[2], corrected_date_components[0], corrected_date_components[1]]\n",
    "    return('-'.join(corrected_date_ordering))\n",
    "\n",
    "lambda_split_date = lambda row: str(row).split(' ')[0]\n",
    "lambda_split_time = lambda row: str(row).split(' ')[1]\n",
    "\n",
    "# Split out the dates into their own column\n",
    "df_trips['StartDate'] = df_trips['Start date'].apply(lambda_split_date)\n",
    "df_trips['EndDate'] = df_trips['End date'].apply(lambda_split_date)\n",
    "\n",
    "# Split out the times into their own column\n",
    "df_trips['StartTime'] = df_trips['Start date'].apply(lambda_split_time)\n",
    "df_trips['EndTime'] = df_trips['End date'].apply(lambda_split_time)\n",
    "\n",
    "# Correct the date formatting\n",
    "df_trips['StartDate'] = df_trips['StartDate'].apply(correct_date_string)\n",
    "df_trips['EndDate'] = df_trips['EndDate'].apply(correct_date_string)\n",
    "\n",
    "# Set the datatypes\n",
    "lambda_convert_minutes = lambda x: int(x/60)\n",
    "\n",
    "df_trips['StartDate'] = df_trips['StartDate'].astype('datetime64[ns]')\n",
    "df_trips['EndDate'] = df_trips['EndDate'].astype('datetime64[ns]')\n",
    "df_trips['Duration'] = df_trips['Total duration (Seconds)'].astype(int).apply(lambda_convert_minutes)\n",
    "df_trips['Start station number'] = df_trips['Start station number'].astype(str)\n",
    "df_trips['End station number'] = df_trips['End station number'].astype(str)\n",
    "\n",
    "# Add month \n",
    "df_trips['StartMonthYear'] = df_trips['StartDate'].apply(lambda_get_date)\n",
    "\n",
    "# Drop unnecessary columnsn and rename poorly named columns\n",
    "df_trips_corrected = df_trips.drop(['Start date', 'End date', 'Total duration (Seconds)', 'Start station', 'End station'], axis = 1)\n",
    "df_trips_corrected = df_trips_corrected.rename(columns = {'Account type': 'AccountType', \n",
    "                                      'Start station number': 'StartStation',\n",
    "                                      'End station number': 'EndStation'})\n",
    "\n",
    "print_df_summary(df_trips_corrected, 'df_trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Trips\n",
    "\n",
    "First, the daily totals, then monthly totals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_total_day = df_trips_corrected.groupby(['StartDate']).size()\n",
    "df_trips_total_day = df_trips_total_day.reset_index()\n",
    "df_trips_total_day.columns = ['StartDate', 'Total']\n",
    "df_trips_total_day['StartDate'] = df_trips_total_day['StartDate'].astype('datetime64[ns]')\n",
    "\n",
    "plot = (ggplot(df_trips_total_day) +\n",
    "        geom_bar(aes(x = 'StartDate', y = 'Total'), \n",
    "                 stat = 'identity',\n",
    "                 fill = 'cornflowerblue') +\n",
    "        ggtitle('2017 Rides Per Day') +\n",
    "        xlab('Date') +\n",
    "        ylab('Total Rides') +\n",
    "        theme(axis_text_x = element_text(rotation = 45, hjust = 1))\n",
    ")\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gaps are pretty interesting, and the distribution looks fairly normal. Unexpectedly, there is no apparent Nice Ride service from December to March. I know people that bike all winter, but I can empathize with the business decision -- Minnesota winters can be very brutal. \n",
    "\n",
    "Let's take a closer look at the gaps. My intuition tells me, perhaps the gaps are on the weekends and that the Nice Ride service is primarily used by commuters during the weekdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_weekday = {0: 'Monday',\n",
    "               1: 'Tuesday',\n",
    "               2: 'Wednesday',\n",
    "               3: 'Thursday',\n",
    "               4: 'Friday',\n",
    "               5: 'Saturday',\n",
    "               6: 'Sunday'}\n",
    "day_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "\n",
    "# Map date to weekday\n",
    "df_trips_total_day['Weekday'] = df_trips_total_day['StartDate'].apply(lambda x: x.weekday()).replace(map_weekday)\n",
    "df_trips_total_weekday = df_trips_total_day.groupby('Weekday')['Total'].sum()\n",
    "\n",
    "# Order days of the week as expected\n",
    "df_trips_total_weekday = df_trips_total_weekday.reset_index()\n",
    "df_trips_total_weekday['Weekday'] = pd.Categorical(df_trips_total_weekday['Weekday'], categories = day_order, ordered = True)\n",
    "\n",
    "plot = (ggplot(df_trips_total_weekday) +\n",
    "        geom_bar(aes(x = 'Weekday', y = 'Total'), \n",
    "                 stat = 'identity',\n",
    "                 color = 'white',\n",
    "                 fill = 'cornflowerblue') +\n",
    "        ggtitle('2017 Rides Per Day of Week') +\n",
    "        xlab('Date') +\n",
    "        ylab('Total Rides') +\n",
    "        theme(axis_text_x = element_text(rotation = 45, hjust = 1))\n",
    ")\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, well that hypothesis was completely wrong. Weekends trend higher than weekdays. Instead, we'll slice out a smaller portion of the total data set to look at the gaps closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_total_slice = df_trips_total_day[df_trips_total_day['StartDate'].between('2017-07-02', '2017-08-15')]\n",
    "\n",
    "plot = (ggplot(df_trips_total_slice) +\n",
    "        geom_bar(aes(x = 'StartDate', y = 'Total'), \n",
    "                 stat = 'identity',\n",
    "                 color = 'white',\n",
    "                 fill = 'cornflowerblue') +\n",
    "        ggtitle('2017 Rides Per Day, Sliced') +\n",
    "        xlab('Date') +\n",
    "        ylab('Total Rides') +\n",
    "        theme(axis_text_x = element_text(rotation = 45, hjust = 1))\n",
    ")\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, perhaps the gaps are just a visual issue with `ggplot`, but it was a fun exercise and some new things were learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_total_day = df_trips_corrected.groupby(['StartMonthYear']).size()\n",
    "df_trips_total_day = df_trips_total_day.reset_index()\n",
    "df_trips_total_day.columns = ['StartDate', 'Total']\n",
    "\n",
    "plot = (ggplot(df_trips_total_day) +\n",
    "        geom_bar(aes(x = 'StartDate', y = 'Total'), \n",
    "                 stat = 'identity',\n",
    "                 color = 'white',\n",
    "                 fill = 'cornflowerblue') +\n",
    "        ggtitle('2017 Rides Per Month') +\n",
    "        xlab('Date') +\n",
    "        ylab('Total Rides') +\n",
    "        theme(axis_text_x = element_text(rotation = 45, hjust = 1))\n",
    ")\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well the summer months certainly get their use, but fall and spring are adequately represented as well, with a few rides in November, brave souls. Now, let's look at the times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the trip start hour, suitable for binning\n",
    "lambda_split_time_hour = lambda x: int(str(x).split(':')[0])\n",
    "df_trips_corrected['Hour'] = df_trips_corrected['StartTime'].apply(lambda_split_time_hour)\n",
    "\n",
    "plot = (ggplot(df_trips_corrected) +\n",
    "            geom_histogram(aes(x = 'Hour'),\n",
    "                          color = 'white',\n",
    "                          fill = 'cornflowerblue',\n",
    "                          binwidth = 2) +\n",
    "        ggtitle('Trip Start Time Histogram') +\n",
    "        xlab('Hour of Day (24)') +\n",
    "        ylab('Frequency')\n",
    "       )\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (ggplot(df_trips_corrected) +\n",
    "            geom_histogram(aes(x = 'Hour'),\n",
    "                          color = 'white',\n",
    "                          fill = 'cornflowerblue',\n",
    "                          binwidth = 2) +\n",
    "        facet_wrap('~StartMonthYear', ncol = 4) +\n",
    "        ggtitle('Trip Start Time Histogram') +\n",
    "        xlab('Hour of Day (24)') +\n",
    "        ylab('Frequency')\n",
    "       )\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like most trips occur around possible lunch breaks. Honestly, I expected more early morning and late afternoon rides. Perhaps people aren't taking Nice Ride bikes as their main commute option but instead, to grab lunch or run a brief errand. Most of the stations in Minneapolis are in the central business district and the University of Minnesota so perhaps that makes sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duration_under_25 = df_trips_corrected[df_trips_corrected['Duration'] < 100]\n",
    "\n",
    "plot = (ggplot(df_duration_under_25) +\n",
    "            geom_histogram(aes(x = 'Duration'),\n",
    "                          color = 'white',\n",
    "                          fill = 'cornflowerblue',\n",
    "                          binwidth = 5) +\n",
    "        ggtitle('Duration Histogram') +\n",
    "        xlab('Duration (Minutes)') +\n",
    "        ylab('Frequency')\n",
    "       )\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I actually found that most rides fell into these buckets. There were a few incredible outliers, including one trip duration that lasted over 130 days. My rough estimate based on the pricing of Nice Ride was about $12,000. Ouch!\n",
    "\n",
    "I tried to bin the outliers below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duration_outliers = df_trips_corrected[(np.abs(df_trips_corrected['Duration'] - df_trips_corrected['Duration'].mean()) > (3 * df_trips_corrected['Duration'].std()))]\n",
    "\n",
    "plot = (ggplot(df_duration_outliers) +\n",
    "            geom_histogram(aes(x = 'Duration'),\n",
    "                          color = 'white',\n",
    "                          fill = 'cornflowerblue',\n",
    "                          bins = 25) +\n",
    "        ggtitle('Duration Histogram (Outliers)') +\n",
    "        xlab('Duration (Minutes)') +\n",
    "        ylab('Frequency')\n",
    "       )\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_start_station_counts = pd.DataFrame(df_trips_corrected.groupby('StartStation').size()).reset_index()\n",
    "df_start_station_counts.columns = ['Number', 'RideCount']\n",
    "\n",
    "# Evenly scale the number of rides to even out the radius for the plot\n",
    "scaler = MinMaxScaler(feature_range = (0.5, 10))\n",
    "scaler.fit(df_start_station_counts[['RideCount']])\n",
    "df_start_station_counts['Scale'] = scaler.transform(df_start_station_counts[['RideCount']])\n",
    "\n",
    "df_start_station_scaled = pd.merge(df_stations_mpls, df_start_station_counts, on = 'Number')\n",
    "build_map(df_start_station_scaled, should_scale = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger the circle, the more popular the station. The popular starting locations appear to be around Bde Maka Ska (the lake in southwest corner), the central business district of Downtown, and the University of Minnesota campus. These align with my expectations as rides can be for pleasure (around the lake), for work (Downtown), and for school (U campus).\n",
    "\n",
    "Let's apply the same treatment to the end stations.\n",
    "\n",
    "### End Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_end_station_counts = pd.DataFrame(df_trips_corrected.groupby('EndStation').size()).reset_index()\n",
    "df_end_station_counts.columns = ['Number', 'RideCount']\n",
    "\n",
    "# Evenly scale the number of rides to even out the radius for the plot\n",
    "scaler = MinMaxScaler(feature_range = (0.5, 10))\n",
    "scaler.fit(df_end_station_counts[['RideCount']])\n",
    "df_end_station_counts['Scale'] = scaler.transform(df_end_station_counts[['RideCount']])\n",
    "\n",
    "df_end_station_scaled = pd.merge(df_stations_mpls, df_end_station_counts, on = 'Number')\n",
    "build_map(df_end_station_scaled, should_scale = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, looks pretty similar to the Start Station results.\n",
    "\n",
    "### Most Popular Route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count stations\n",
    "df_popular_trips = pd.DataFrame(df_trips_corrected.groupby(['StartStation', 'EndStation']).size()).reset_index(drop = False)\n",
    "df_popular_trips.columns = ['StartStationNumber', 'EndStationNumber', 'RouteCount']\n",
    "\n",
    "# Swap out station numbers with names\n",
    "map_station_names = df_stations.set_index('Number').to_dict()['Name']\n",
    "df_popular_trips['StartStation'] = df_popular_trips['StartStationNumber'].replace(map_station_names)\n",
    "df_popular_trips['EndStation'] = df_popular_trips['EndStationNumber'].replace(map_station_names)\n",
    "\n",
    "# Get top ten\n",
    "df_popular_trips_10 = df_popular_trips.nlargest(10, 'RouteCount').reset_index(drop = True)\n",
    "\n",
    "# Set datatypes\n",
    "df_popular_trips_10 = df_popular_trips_10.reset_index(drop = True)\n",
    "df_popular_trips_10['RouteCount'] = df_popular_trips_10['RouteCount'].astype(int)\n",
    "\n",
    "# Clean up dataframe\n",
    "df_popular_trips_10 = df_popular_trips_10.drop(['StartStationNumber', 'EndStationNumber'], axis = 1)\n",
    "df_popular_trips_10 = df_popular_trips_10.sort_values(by = ['RouteCount'], ascending = False)\n",
    "df_popular_trips_10.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very interesting! The most popular routes ended up being a loop of some sort. There could be many explainations for this, here's a few that I considered: \n",
    "\n",
    "* Popular routes are around activities such as lakes, so individual commutes around town get lost.\n",
    "* There is some sort of anxiety around finding another docking station, so people run their errand and return it where they got it from.\n",
    "* People use public transportation or personal car to reach a 'hub point,' then use Nice Ride bikes to perform some sort of local errand. \n",
    "\n",
    "Let's try to find popular stops that had a different starting station from the ending station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_different_popular_trips = df_popular_trips[df_popular_trips['StartStation'] != df_popular_trips['EndStation']]\n",
    "df_different_popular_trips = df_different_popular_trips.sort_values(by = 'RouteCount', ascending = False)\n",
    "df_different_popular_trips = df_different_popular_trips.drop(['StartStationNumber', 'EndStationNumber'], axis = 1)\n",
    "df_different_popular_trips.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the basic EDA complete, let's jump into the modeling and answering the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Question 1:  Do stations align with neighboorhoods?\n",
    "\n",
    "Minneapolis isn't as neighboorhood focused as San Francisco, but it does have it's regions. I thought it would interesting to check for clusters of stations and see if they align with the fuzzy boundaries of Minneapolis neighboorhoods. I started with KMeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_locations = df_stations_mpls[['Latitude', 'Longitude']]\n",
    "model_cluster = KMeans(n_clusters = 5, init = 'random')\n",
    "model_cluster.fit_transform(df_stations_locations)\n",
    "\n",
    "df_stations_mpls['Cluster'] = model_cluster.labels_\n",
    "build_map(df_stations_mpls, should_cluster = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, this works pretty well. Uptown (blue), Downtown (Purple), South (Yellow) and the University of Minnesota campus (Red) are fairly aligned. North Minneapolis (green) is mostly correct, but some of the green, purple, and red points to the east of the Mississippi river should be the neighborhood of Northeast. I wonder if adding more clusters would get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_locations = df_stations_mpls[['Latitude', 'Longitude']]\n",
    "model_cluster = KMeans(n_clusters = 8, init = 'random')\n",
    "model_cluster.fit_transform(df_stations_locations)\n",
    "\n",
    "df_stations_mpls['Cluster'] = model_cluster.labels_\n",
    "build_map(df_stations_mpls, should_cluster = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With eight clusters, the neighborhoods are almost perfect. Some of the blue points south of the river should be orange, and the white points that are east of the pink points should be pink, for downtown. Pretty close though!\n",
    "\n",
    "Let's try a different clustering algorithm. I feel like `dbscan` is the way to go here since I want neighborhoods that exist organically to appear. There are some stations, like the one farthest to the north that I personally would classify as \"unclustered\" which is something `dbscan` supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distance, math from:\n",
    "# https://geoffboeing.com/2014/08/clustering-to-reduce-spatial-data-set-size/\n",
    "km_per_radian = 6371.0088\n",
    "epsilon = 1.2 / km_per_radian\n",
    "\n",
    "df_stations_locations = df_stations_mpls[['Latitude', 'Longitude']]\n",
    "model_dbscan = DBSCAN(eps = epsilon, min_samples = 2, metric = 'haversine', algorithm = 'ball_tree')\n",
    "labels = model_dbscan.fit_predict(np.radians(np.array(df_stations_locations)))\n",
    "\n",
    "labels = np.absolute(labels)\n",
    "df_stations_mpls['Cluster'] = labels\n",
    "build_map(df_stations_mpls, should_cluster = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result ended up being the best I could do with many iterations and attempts to find appropriate epsilon and minimum samples. The result isn't very great. How about one more try with one more clustering method: Spectral Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_locations = df_stations_mpls[['Latitude', 'Longitude']]\n",
    "\n",
    "model_specrtral = SpectralClustering(n_clusters = 7, \n",
    "                                     assign_labels = 'discretize', \n",
    "                                     n_init = 50,\n",
    "                                     affinity = 'nearest_neighbors',\n",
    "                                     random_state = 74656)\n",
    "labels = model_specrtral.fit_predict(df_stations_locations)\n",
    "\n",
    "df_stations_mpls['Cluster'] = labels\n",
    "build_map(df_stations_mpls, should_cluster = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately I think that this is the best outcome I could have hoped for. The inner downtown in pink is separated from the outer downtown in blue. A few of the Northeast stations (purple) have been misclassified as yellow for North Minneapolis. But the University of Minnesota stations (green), South Minneapolis (orange) and Uptown (red)stations are nearly perfect. \n",
    "\n",
    "To answer the original question, yes, I do think stations map to established neighborhoods.\n",
    "\n",
    "### Question 2: Can predictions about the number of rides per day based weather be accomplished with 80% or more accuracy?\n",
    "\n",
    "I'll start by plotting a correlation to see how the total number of rides with the other variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_total_day = pd.DataFrame(df_trips_corrected.groupby('StartDate').size().reset_index())\n",
    "df_trips_total_day.columns = ['DATE', 'TotalTrips']\n",
    "df_trips_by_weather = pd.merge(df_trips_total_day, df_weather, on = 'DATE').drop(['STATION', 'MONTH', 'NAME'], axis = 1)\n",
    "\n",
    "# Split data into features and labels\n",
    "df_X = df_trips_by_weather[['PRCP', 'TMAX', 'TMIN']]\n",
    "df_y = df_trips_by_weather[['TotalTrips']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation code from:\n",
    "# https://stackoverflow.com/questions/29432629/correlation-matrix-using-pandas#29432741\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "corr = df_trips_by_weather.corr()\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are naturally strong correlations between the minimum and maximum temperature. There is also a medium correlation between temperature and precipitation. As for their effect on rides, it appears that there is a medium positive correlation with temperature and rides. Makes sense, warm days make a bike ride seem very appealing. There's also a fairly strong negative correlation with precipitation and number of rides. It turns out, it's not fun to bike in the rain. \n",
    "\n",
    "To me, this looks like it would be decent at making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor(df_X, df_y):\n",
    "    '''\n",
    "    Builds a regression model, trains it, tests it, and returns all calculations.\n",
    "    \n",
    "    Parameters:\n",
    "    df_X (dataframe): The x values used in the regression\n",
    "    df_y (dataframe): The target y labels\n",
    "    \n",
    "    Returns:\n",
    "    dataframe: (predictions) predictions found with the test set\n",
    "    dataframe: (X_test) the values split out from the training set used to calculate the predictions\n",
    "    dataframe: (y_test) the labels split out from the training set used to calculate the predictions\n",
    "    '''\n",
    "    # Code adapted from\n",
    "    # https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, \n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 74656)\n",
    "\n",
    "\n",
    "    regressor = RandomForestRegressor(n_estimators = 5000, random_state = 74656)\n",
    "    regressor.fit(X_train, y_train);\n",
    "    predictions = regressor.predict(X_test)\n",
    "\n",
    "    errors = abs(predictions - y_test['TotalTrips'])\n",
    "    mape = 100 * (errors / y_test['TotalTrips'])\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "\n",
    "    print('Accuracy: ' + str(round(accuracy, 2)) + '%.')\n",
    "    return(predictions, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had never used a regression before, so I found a nice [tutorial](https://towardsdatascience.com/random-forest-in-python-24d0893d51c0) that helped me work with my dataset. I wrote a method to calculate the accuracy of a `RandomForestRegressor`. The first dataset I used was the original set, which includes precipitation, minimum temperature, and maximum temperature with the number of rides that day as the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, X_test, y_test = regressor(df_X, df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['Actual'] = y_test['TotalTrips']\n",
    "X_test['Predicted'] = predictions\n",
    "X_test = X_test.reset_index(drop = True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (ggplot(X_test) +\n",
    "        geom_line(aes(x = 'index',\n",
    "                      y = 'Actual'),\n",
    "                  color = 'red',\n",
    "                  alpha = 1.0\n",
    "                 ) +\n",
    "        geom_point(aes(x = 'index',\n",
    "                       y = 'Predicted'),\n",
    "                   color = 'blue',\n",
    "                  alpha = 0.5\n",
    "                  ) +\n",
    "        ggtitle('Actual Rides vs Predictions') +\n",
    "        xlab('Position') +\n",
    "        ylab('Number of Rides')\n",
    ")\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, that's not very accurate. I wonder if adding a new feature for day of the week would improve results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up trips\n",
    "df_trips_total_day = pd.DataFrame(df_trips_corrected.groupby('StartDate').size().reset_index())\n",
    "df_trips_total_day.columns = ['DATE', 'TotalTrips']\n",
    "\n",
    "# Add day of week feature\n",
    "df_trips_total_day['DAYOFWEEK'] = df_trips_total_day['DATE'].apply(lambda x: x.weekday()).replace(map_weekday)\n",
    "\n",
    "# Merge datasets\n",
    "df_trips_by_weather = pd.merge(df_trips_total_day, df_weather, on = 'DATE').drop(['STATION', 'MONTH', 'NAME'], axis = 1)\n",
    "\n",
    "# Convert day of week feature to dummy\n",
    "df_day_of_week_dummies = pd.get_dummies(df_trips_by_weather['DAYOFWEEK']).reset_index(drop = True).reset_index()\n",
    "df_X = df_trips_by_weather[['PRCP', 'TMAX', 'TMIN']].reset_index(drop = True).reset_index()\n",
    "\n",
    "# Split out into features and labels\n",
    "df_X = pd.merge(df_X, df_day_of_week_dummies, on = 'index')\n",
    "df_X = df_X.drop('index', axis = 1)\n",
    "df_y = df_trips_by_weather[['TotalTrips']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, X_test, y_test = regressor(df_X, df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['Actual'] = y_test['TotalTrips']\n",
    "X_test['Predicted'] = predictions\n",
    "X_test = X_test.reset_index(drop = True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (ggplot(X_test) +\n",
    "        geom_line(aes(x = 'index',\n",
    "                      y = 'Actual'),\n",
    "                  color = 'red',\n",
    "                  alpha = 1.0\n",
    "                 ) +\n",
    "        geom_point(aes(x = 'index',\n",
    "                       y = 'Predicted'),\n",
    "                   color = 'blue',\n",
    "                  alpha = 0.5\n",
    "                  ) +\n",
    "        ggtitle('Actual Rides vs Predictions') +\n",
    "        xlab('Position') +\n",
    "        ylab('Number of Rides')\n",
    ")\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, that's not too much better. Let's try adding a feature to represent the season. I tried to map the seasons to how they actually *feel* in Minnesota and not how they align with the actual season. For example, typically Novembers in Minnesota can have below freezing temperatures and snow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link month of the 'feeling' of the season\n",
    "map_season = {\n",
    "    1: 'Winter',\n",
    "    2: 'Winter',\n",
    "    3: 'Spring',\n",
    "    4: 'Spring',\n",
    "    5: 'Spring',\n",
    "    6: 'Summer',\n",
    "    7: 'Summer',\n",
    "    8: 'Summer',\n",
    "    9: 'Fall',\n",
    "    10: 'Fall',\n",
    "    11: 'Winter',\n",
    "    12: 'Winter'\n",
    "}\n",
    "\n",
    "# Sum up trips\n",
    "df_trips_total_day = pd.DataFrame(df_trips_corrected.groupby('StartDate').size().reset_index())\n",
    "df_trips_total_day.columns = ['DATE', 'TotalTrips']\n",
    "\n",
    "# Pull out the month from the date column\n",
    "lambda_get_month = lambda x: int(str(x).split('-')[1])\n",
    "df_trips_total_day['MONTH'] = df_trips_total_day['DATE'].apply(lambda_get_month)\n",
    "\n",
    "# Convert the month extracted above to a season\n",
    "df_trips_total_day['SEASON'] = df_trips_total_day['MONTH'].map(map_season)\n",
    "df_trips_total_day = df_trips_total_day.drop('MONTH', axis = 1)\n",
    "df_trips_by_weather = pd.merge(df_trips_total_day, df_weather, on = 'DATE').drop(['STATION', 'MONTH', 'NAME'], axis = 1)\n",
    "\n",
    "# Convert season feature to dummies\n",
    "df_season_dummies = pd.get_dummies(df_trips_by_weather['SEASON']).reset_index(drop = True).reset_index()\n",
    "df_X = df_trips_by_weather[['PRCP', 'TMAX', 'TMIN']].reset_index(drop = True).reset_index()\n",
    "\n",
    "# Split out into features and labels\n",
    "df_X = pd.merge(df_X, df_season_dummies, on = 'index')\n",
    "df_X = df_X.drop('index', axis = 1)\n",
    "df_y = df_trips_by_weather[['TotalTrips']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, X_test, y_test = regressor(df_X, df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['Actual'] = y_test['TotalTrips']\n",
    "X_test['Predicted'] = predictions\n",
    "X_test = X_test.reset_index(drop = True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (ggplot(X_test) +\n",
    "        geom_line(aes(x = 'index',\n",
    "                      y = 'Actual'),\n",
    "                  color = 'red',\n",
    "                  alpha = 1.0\n",
    "                 ) +\n",
    "        geom_point(aes(x = 'index',\n",
    "                       y = 'Predicted'),\n",
    "                   color = 'blue',\n",
    "                  alpha = 0.5\n",
    "                  ) +\n",
    "        ggtitle('Actual Rides vs Predictions') +\n",
    "        xlab('Position') +\n",
    "        ylab('Number of Rides')\n",
    ")\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question, it looks like we can predict the total number of trips based soley on precipitation, maximum temp, and minimum temp to about 63% accuracy by adding a day of the weak feature. That's not particularly great. Perhaps more data is required to get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:  Can the speed of trips between stations be calculated?\n",
    "\n",
    "For the final question, I wanted to calculate the speed that riders take. Of course to do this, I must use trip records where people ended at a different location than where they started. Without knowing the exact route auser took, I can approximate by performing an 'as the crow flies' estimate between the starting and ending locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find trips with different start and ending locations and select only the columns we want\n",
    "df_trips_different_start_end = pd.DataFrame(df_trips_corrected[df_trips_corrected['StartStation'] != df_trips_corrected['EndStation']])\n",
    "df_trips_different_start_end = df_trips_different_start_end.drop(['AccountType', 'StartDate', 'EndDate', 'StartTime', 'EndTime', 'StartMonthYear', 'Hour'], axis = 1)\n",
    "\n",
    "# Break out different tables to match up with coordates\n",
    "df_trips_start = df_trips_different_start_end.drop(['EndStation', 'Duration'], axis = 1)\n",
    "df_trips_start.columns = ['Number']\n",
    "\n",
    "df_trips_end = df_trips_different_start_end.drop(['StartStation', 'Duration'], axis = 1)\n",
    "df_trips_end.columns = ['Number']\n",
    "\n",
    "# Drop station columns we don't need\n",
    "df_stations_simplified = df_stations_mpls.drop(['Name', 'Total docks', 'Cluster'], axis = 1)\n",
    "\n",
    "# Combine separated tables with station locations\n",
    "df_trips_start = pd.merge(df_trips_start, df_stations_simplified, on = 'Number')\n",
    "df_trips_start.columns = ['StartStation', 'StartLat', 'StartLon']\n",
    "\n",
    "df_trips_end = pd.merge(df_trips_end, df_stations_simplified, on = 'Number')\n",
    "df_trips_end.columns = ['EndStation', 'EndLat', 'EndLon']\n",
    "\n",
    "# Combine separted dataframes into one \n",
    "df_trips_start_end = pd.merge(df_trips_start, df_trips_end, left_index = True, right_index = True)\n",
    "df_trips_start_end = df_trips_start_end.drop(['StartStation', 'EndStation'], axis = 1)\n",
    "df_trips_different_start_end = pd.merge(df_trips_different_start_end, df_trips_start_end, left_index = True, right_index = True)\n",
    "\n",
    "df_trips_different_start_end.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the start and stop coordinates put together, along with the duration of the trip, I can use a formula to calculate the distance in kilometers with a formula I found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_between_points(row):\n",
    "    '''\n",
    "    Calculates the distance in kilometers between two latitude and longitude coordinates. \n",
    "    Intended to be applied to an entire row.\n",
    "    \n",
    "    Parameters:\n",
    "    row (dataframe): A dataframe row with the following columns: StartLat, StartLon, EndLat, EndLon\n",
    "    \n",
    "    Returns:\n",
    "    float: Distance in kilometers between the two points\n",
    "    '''\n",
    "    \n",
    "    # Formula adapted from:\n",
    "    # https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude#19412565\n",
    "    \n",
    "    coords1 = (row['StartLat'], row['StartLon'])\n",
    "    coords2 = (row['EndLat'], row['EndLon'])\n",
    "\n",
    "    distance = geopy.distance.vincenty(coords1, coords2).km\n",
    "    \n",
    "    return(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can take a bit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_different_start_end['DistanceKM'] = df_trips_different_start_end.apply(dist_between_points, axis = 1)\n",
    "df_trips_different_start_end.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the distances calculated, I can now calculate KPH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kph(row):\n",
    "    duration_hours = float(row['DistanceKM'])/float(row['Duration'] / 60)\n",
    "    return(duration_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_different_start_end['KPH'] = df_trips_different_start_end.apply(calculate_kph, axis = 1)\n",
    "df_trips_different_start_end.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I already know there are some ugly outliers with `Duration` from the EDA. Let's try to drop these as they may have negatively impacted the KPH calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_index = df_trips_different_start_end['KPH'].quantile(0.95)\n",
    "df_trips_distance_kph = df_trips_different_start_end[df_trips_different_start_end['KPH'] < quantile_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With everything calculated, let's take a look at some metrics! Starting with a histogram for distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (ggplot(df_trips_distance_kph) +\n",
    "            geom_histogram(aes(x = 'DistanceKM'),\n",
    "                          color = 'white',\n",
    "                          fill = 'cornflowerblue',\n",
    "                          bins = 10) +\n",
    "        ggtitle('Distance Histogram') +\n",
    "        xlab('Distance (KM)') +\n",
    "        ylab('Frequency')\n",
    "       )\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a skewed distribution around 1 kilometers, but some higher. Next let's look at KPH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = (ggplot(df_trips_distance_kph) +\n",
    "            geom_histogram(aes(x = 'KPH'),\n",
    "                          color = 'white',\n",
    "                          fill = 'cornflowerblue',\n",
    "                          bins = 20) +\n",
    "        ggtitle('KPH Histogram') +\n",
    "        xlab('KPH') +\n",
    "        ylab('Frequency')\n",
    "       )\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these are some very fast bikers! Let's take a look at the top trip from this selection to see where to see what kind of trip this is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_speed = df_trips_distance_kph.sort_values(by = ['KPH']).tail(1)\n",
    "df_top_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_row = [df_top_speed['StartLat'].iloc[0], df_top_speed['StartLon'].iloc[0]]\n",
    "end_row = [df_top_speed['EndLat'].iloc[0], df_top_speed['EndLon'].iloc[0]]\n",
    "start_end = [start_row, end_row]\n",
    "\n",
    "df_top_speed_map = pd.DataFrame(start_end)\n",
    "df_top_speed_map.columns = ['Latitude', 'Longitude']\n",
    "build_map(df_top_speed_map, is_simple = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow I find it hard to believe that someone could really bike 70 kilometers per hour between these two stops. Perhaps someone loaded the bike onto a vehicle? It's hard to say. Perhaps I calculated duration and distance incorrectly, or the data isn't 100% accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "There weren't many features in this dataset, and yet, some interesting questions can be asked and results found. I asked three questions of this dataset, let's summarize the results:\n",
    "\n",
    "* Do station locations map to established neighborhoods?\n",
    "\n",
    "Yes! At least at a first glance, some clustering algorithms come up with clusters that align very closely with actual neighboorhoods. This could be because stations and neighborhoods are based on geographic boundaries. It could also be that neighboorhoods and stations follow patterns of density. Results here could be improved if hard boundaries, like a river, could be fed into the clustering algorithm. This would prevent the few stations that were incorrectly clustered.\n",
    "\n",
    "* Can the total number of trips be predicted based on weather data to 80% accuracy?\n",
    "\n",
    "No! At least not with the data provided. The regression I used in addition to several engineered features could only get to about 60% accuracy. Perhaps more data is required.\n",
    "\n",
    "* Can the speed of a trip be calculated?\n",
    "\n",
    "Yes! Well, sort of! Without knowing the actual route someone took, my 'best guess' is to calculate the distance between two stations as a straight line. While not completely accurate, it does provide a decent estimation and should be internally consistent enough for start-end station pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
